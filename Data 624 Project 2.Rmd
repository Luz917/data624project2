---
title: "Data 624 Project 2"
author: "Maryluz Cruz, Bill Stepniak, Sherranette Tinapunan"
date: "5/11/2021"
output:
  html_document:
    df_print: paged
    theme: cerulean
    #code_folding: hide
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(mice)
library(caret)
library(e1071)
library(psych)
library(DataExplorer)
library(RANN)
library(MASS)
library(ggplot2)
library(elasticnet)
library(randomForest)
library(skimr)
library(DataExplorer)
library(GGally)
library(corrplot)
```


## 1. Problem Statement

This is role playing. I am your new boss. I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me. My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH. Please use the historical data set I am providing. Build and report the factors in BOTH a technical and non-technical report. I like to use Word and Excel. Please provide your non-technical report in a business friendly readable document and your predictions in an Excel readable format. The technical report should show clearly the models you tested and how you selected your final approach. Please submit both RPubs links and .rmd files or other readable formats for technical and non-technical reports. Also submit the excel file showing the prediction of your models for pH.

---

## 2. Load Data 

Two files are provided for this project. The file `StudentData.csv` contains the data used to build the predictive models for <i>PH</i>. The file `StudentEvaluation.csv` contains the data used to evaluate the selected model. The <i>PH</i> column in this file has no data.

```{r}
student_data<-read.csv("https://raw.githubusercontent.com/Luz917/data624project2/master/StudentData.csv")
student_eval<-read.csv("https://raw.githubusercontent.com/Luz917/data624project2/master/StudentEvaluation.csv")
```

### 2.2. Summary of Student Data

The `Student Data` data set has 2,571 observations and 33 variables. 

The data set contains categorical, continuous, and discrete variables. The data set has some missing values for most of the variables. The response variable <i>PH</i> has 4 missing values. All the predictor variables have missing values except for <i>Pressure.Vacuum</i> and <i>Air.Pressurer</i>. The categorical variable, <i>Brand.Code</i> has 120 missing values. 

```{r}
#summary(student_data)
skim(student_data)
```

The variable <i>Brand.Code</i> has four different categories, and there are 120 observations with categories that are not known. 

```{r}
table(student_data$ï..Brand.Code)
```

### 2.3. Summary of Student Evaluation 

The `Student Evaluation` data has 267 observations with 33 variables. 

As you can see, the data set also has some missing values for most of the variables. The response variable, <i>PH</i>, has no value as this will be predicted by the selected model. The categorical variable, <i>Brand.Code</i>, has 4 missing values. 

```{r}
#summary(student_eval)
skim(student_eval)
```

The variable <i>Brand.Code</i> of the evaluation data set has four different categories and eight observations without a known category. 

```{r}
table(student_eval$ï..Brand.Code)
```

Doing a little cleanup of the variable name `i..Brand.Code` and simply renaming this to `Brand.Code`. 

```{r}
names(student_data)[names(student_data) == "ï..Brand.Code"] <- "Brand.Code"
names(student_eval)[names(student_eval) == "ï..Brand.Code"] <- "Brand.Code"
```

---

## 3.Data Exploration

### 3.x Disribution of Response Variable <i>PH</i> 

The distribution of the response variable <i>PH</i> is approximately normal with some degree of skewness to the left. The mean <i>PH</i> value is 8.5. We have fewer observations on the extremes of the distribution, which could mean that the models are not able to predict PH values that fall towards the edge of the distribution as we have less information to tune the models. 

```{r}
hist(student_data$PH)
```

### 3.1. Distribution of Predictors

Below shows the distribution of the numeric predictors. We can see that some variables have bimodal features (e.g., `Air Pressure`,  `Hyd.Pressure2`, `Hyd.Pressure3`, `Balling`). This is very interesting as this could suggests the existence of different groups, which could have different distributions. Some distributions suggests a more discrete features with a much more limited set of distinct values such as `Pressure Setpoint`, `PSC.CO2`, and `Bowl.Setpoint`. 

```{r}
plot_histogram((student_data[-c(26)]))
```


### 3.2. Pairwise Scatter plot

Below is a pairwise plot of the predictor variables versus the response variable <i>PH</i>. This should give us an idea of the predictors' relationship to the response variable. 


```{r fig.height=13, fig.width=13, message=FALSE, warning=FALSE, echo=FALSE}

pairs(student_data[c(26,2:11)], col="grey40")
pairs(student_data[c(26,12:22)], col="grey40")
pairs(student_data[c(26,23:25,27:33)], col="grey40")
```

### 3.3 Correlation

Below is heat map of correlations between variables. As you can see, some variables are more strongly correlated than others. Response variable `PH` is more strongly correlated to `Mnf.Flow`.  `Hyd.Pressure3` has a somewhat moderate negative correlation with `Pressure.Vaccuum`. There are some variables with significant positive correlations. `Balling` is strongly correlated with `Balling.Lv1` (0.98), among others. Concerns about multicollinearity should be considered when selecting features for our models. We should avoid including pairs that are strongly correlated with each other. 


```{r fig.height=13, fig.width=13, message=FALSE, warning=FALSE, echo=FALSE}
corr_data =cor(student_data[c(26,2:25,27:33)], use="pairwise.complete.obs", method = "pearson")
corrplot(corr_data, method = "color",type = "upper", order = "original", number.cex = .7,addCoef.col = "black",   #Add coefficient of correlation
                            tl.srt = 90,# Text label color and rotation
                            diag = TRUE)# hide correlation coefficient on the principal diagonal
```


### 3.4 Box Plot

Below is a scaled box plots of the variables. As you can see, some of the variables have outliers. 

```{r fig.height=13, fig.width=13, message=FALSE, warning=FALSE, echo=FALSE}
x <- data.frame(scale(na.omit(student_data[c(2:33)])))
ggplot(stack(x), aes(x= ind, y = values)) + 
  geom_boxplot(outlier.colour="green", outlier.shape=1, outlier.size=2, aes(fill=ind)) + theme_minimal() + coord_flip() 
```


### 3.5 Missing Data 

Notice that approximately 8.25 percent of the rows are missing a value for MFR. We may need to drop this feature considering that, as missingness increases, so do the potential negative consequences of imputation. Additionally, the categorical feature Brand Code is missing approximately 4.67 percent of its values. Since we do not know whether these values represent another brand or are actually missing, we will create a new feature category ‘Unknown’ consisting of missing values. The rest of the features are only missing small percentages of values, suggesting that KNN imputation should be safe.

As you can see, approximately 8.25 percent of the rows are missing values for `MFR` with a count of 212 missing values. Bias is likely in analyses when missingness is more than 10%. Earlier data exploration revealed that the categorical variable `Brand.Code` has missing values. However, the `plot_missing` function did not reflect this, which originally showed a 0% value for missingness as the missing data are not represented by the value `NA`. To reflect the accurate missing ratio of `Brand.Code`, the value `NA` was assigned. Approximately, 4.67 percent of rows are missing for `Brad.Code`.


```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE, echo=FALSE}
https://www.sciencedirect.com/science/article/pii/S0895435618308710
plot_missing(student_data)
```



## 4. Data Preparation 

### Prepare and Create Dummy Variables of Brand.Code

```{r}
# student_data
Brand.A <- ifelse(student_data$Brand.Code  == 'A', 1, 0)
Brand.B <- ifelse(student_data$Brand.Code  == 'B', 1, 0)
Brand.C <- ifelse(student_data$Brand.Code  == 'C', 1, 0)
Brand.D <- ifelse(student_data$Brand.Code  == 'D', 1, 0)
student_data <- subset(student_data, select = -c(Brand.Code))

# student_eval
Brand.A.t <- ifelse(student_eval$Brand.Code  == 'A', 1, 0)
Brand.B.t <- ifelse(student_eval$Brand.Code  == 'B', 1, 0)
Brand.C.t <- ifelse(student_eval$Brand.Code == 'C', 1, 0)
Brand.D.t <- ifelse(student_eval$Brand.Code  == 'D', 1, 0)
student_eval <- subset(student_eval, select = -c(Brand.Code))


# create dummy columns 
student_data$Brand.A <- Brand.A
student_data$Brand.B <- Brand.B
student_data$Brand.C <- Brand.C
student_data$Brand.D <- Brand.D

student_eval$Brand.A <- Brand.A.t
student_eval$Brand.B <- Brand.B.t
student_eval$Brand.C <- Brand.C.t
student_eval$Brand.D <- Brand.D.t
```


Impute Data and Deal with Normality and NearZero 

```{r}
# pre-processing - combine all steps
prep <- preProcess(student_data, 
                   method = c("knnImpute", "center", "scale", "nzv"), k = 10)

student_data <- predict(prep, student_data)
student_eval <- predict(prep, student_eval)
```


```{r}
# Train set
highcor = findCorrelation(cor(student_data), 0.85)

student_data = student_data[, -highcor]
```

  


























